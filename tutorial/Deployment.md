# Deploy your ML models

## Issues



## On the Server 

* Buffering
* parallel 
* Batch Inference

### Microservices (logging, metrics, Outliers, Drift, Explainers & Statistical Performance)

* Streamlit/Dash

* Flask/Django/FastAPI

* Multi-model-server

* Tensorflow Serving/Pytorch Serving

* Seldon Core

#### On Cloud Platform (serverless)

* GCP
* AWS
* Azure

### Embedded (Your model is not served as a microservice, but embedded in the app)

* [Intel(R) Deep Learning Streamer Pipeline Server](https://github.com/dlstreamer/pipeline-server)

* [Mediapipe]()

* [Kafka Stream]()

## Edge

### Mobile Device

#### Mediapipe

### Raspberry Pi
