{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pipeline_bike_gcp.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNW7sJ9DCnSVqoSyKZxGF2n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/epadam/production-level-data-analysis/blob/master/pipeline/exp/pipeline_bike_gcp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbd9tTFgHPDb"
      },
      "source": [
        "#### Utility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrIOGoGZGVjG"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "from typing import Tuple\n",
        "\n",
        "import absl\n",
        "import numpy as np\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from tfx.components.trainer.fn_args_utils import DataAccessor\n",
        "from tfx.components.trainer.fn_args_utils import FnArgs\n",
        "from tfx.dsl.io import fileio\n",
        "from tfx.utils import io_utils\n",
        "from tfx_bsl.tfxio import dataset_options\n",
        "\n",
        "from tensorflow_metadata.proto.v0 import schema_pb2\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_transform as tft\n",
        "\n",
        "_FEATURE_KEYS = [\n",
        "   'season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed'\n",
        "]\n",
        "_LABEL_KEY = 'cnt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7xyue_CHj5N"
      },
      "source": [
        "_TRAIN_DATA_SIZE = 658\n",
        "_TRAIN_BATCH_SIZE = 20\n",
        "\n",
        "def _input_fn(\n",
        "    file_pattern: str,\n",
        "    data_accessor: DataAccessor,\n",
        "    schema: schema_pb2.Schema,\n",
        "    batch_size: int = 20,\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "  \"\"\"Generates features and label for tuning/training.\n",
        "  Args:\n",
        "    file_pattern: input tfrecord file pattern.\n",
        "    data_accessor: DataAccessor for converting input to RecordBatch.\n",
        "    schema: schema of the input data.\n",
        "    batch_size: An int representing the number of records to combine in a single\n",
        "      batch.\n",
        "  Returns:\n",
        "    A (features, indices) tuple where features is a matrix of features, and\n",
        "      indices is a single vector of label indices.\n",
        "  \"\"\"\n",
        "  record_batch_iterator = data_accessor.record_batch_factory(\n",
        "      file_pattern,\n",
        "      dataset_options.RecordBatchesOptions(batch_size=batch_size, num_epochs=1),\n",
        "      schema)\n",
        "\n",
        "  feature_list = []\n",
        "  label_list = []\n",
        "  for record_batch in record_batch_iterator:\n",
        "    record_dict = {}\n",
        "    for column, field in zip(record_batch, record_batch.schema):\n",
        "      record_dict[field.name] = column.flatten()\n",
        "\n",
        "    label_list.append(record_dict[_LABEL_KEY])\n",
        "    features = [record_dict[key] for key in _FEATURE_KEYS]\n",
        "    feature_list.append(np.stack(features, axis=-1))\n",
        "\n",
        "  return np.concatenate(feature_list), np.concatenate(label_list)\n",
        "\n",
        "\n",
        "# TFX Trainer will call this function.\n",
        "def run_fn(fn_args: FnArgs):\n",
        "  \"\"\"Train the model based on given args.\n",
        "  Args:\n",
        "    fn_args: Holds args used to train the model as name/value pairs.\n",
        "  \"\"\"\n",
        "  schema = io_utils.parse_pbtxt_file(fn_args.schema_file, schema_pb2.Schema())\n",
        "\n",
        "  x_train, y_train = _input_fn(fn_args.train_files, fn_args.data_accessor,\n",
        "                               schema)\n",
        "  x_eval, y_eval = _input_fn(fn_args.eval_files, fn_args.data_accessor, schema)\n",
        "\n",
        "  steps_per_epoch = _TRAIN_DATA_SIZE / _TRAIN_BATCH_SIZE\n",
        "\n",
        "  estimator = MLPClassifier(\n",
        "      hidden_layer_sizes=[8, 8, 8],\n",
        "      activation='relu',\n",
        "      solver='adam',\n",
        "      batch_size=_TRAIN_BATCH_SIZE,\n",
        "      learning_rate_init=0.0005,\n",
        "      max_iter=int(fn_args.train_steps / steps_per_epoch),\n",
        "      verbose=True)\n",
        "\n",
        "  # Create a pipeline that standardizes the input data before passing it to an\n",
        "  # estimator. Once the scaler is fit, it will use the same mean and stdev to\n",
        "  # transform inputs at both training and serving time.\n",
        "  model = Pipeline([\n",
        "      ('scaler', StandardScaler()),\n",
        "      ('estimator', estimator),\n",
        "  ])\n",
        "  model.feature_keys = _FEATURE_KEYS\n",
        "  model.label_key = _LABEL_KEY\n",
        "  model.fit(x_train, y_train)\n",
        "  absl.logging.info(model)\n",
        "\n",
        "  score = model.score(x_eval, y_eval)\n",
        "  absl.logging.info('Accuracy: %f', score)\n",
        "\n",
        "  # Export the model as a pickle named model.pkl. AI Platform Prediction expects\n",
        "  # sklearn model artifacts to follow this naming convention.\n",
        "  os.makedirs(fn_args.serving_model_dir)\n",
        "\n",
        "  model_path = os.path.join(fn_args.serving_model_dir, 'model.pkl')\n",
        "  with fileio.open(model_path, 'wb+') as f:\n",
        "    pickle.dump(model, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhHHYeswH7Df"
      },
      "source": [
        "### tfx pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WM_24bI_Hzj6"
      },
      "source": [
        "import os\n",
        "from typing import List\n",
        "\n",
        "import absl\n",
        "import tensorflow_model_analysis as tfma\n",
        "from tfx.components import CsvExampleGen\n",
        "from tfx.components import Evaluator\n",
        "from tfx.components import ExampleValidator\n",
        "from tfx.components import Pusher\n",
        "from tfx.components import SchemaGen\n",
        "from tfx.components import StatisticsGen\n",
        "from tfx.components import Trainer\n",
        "from tfx.components import Transform\n",
        "from tfx.dsl.components.common import resolver\n",
        "from tfx.dsl.experimental import latest_blessed_model_resolver\n",
        "from tfx.orchestration import metadata\n",
        "from tfx.orchestration import pipeline\n",
        "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
        "from tfx.proto import pusher_pb2\n",
        "from tfx.proto import trainer_pb2\n",
        "from tfx.types import Channel\n",
        "from tfx.types.standard_artifacts import Model\n",
        "from tfx.types.standard_artifacts import ModelBlessing\n",
        "\n",
        "# Identifier for the pipeline. This will also be used as the model name on AI\n",
        "# Platform, so it should begin with a letter and only consist of letters,\n",
        "# numbers, and underscores.\n",
        "_pipeline_name = 'bike_sklearn_gcp'\n",
        "\n",
        "# Google Cloud Platform project id to use when deploying this pipeline. Leave\n",
        "# blank to run locally.\n",
        "_project_id = 'PROJECT_ID'\n",
        "\n",
        "# Directory and data locations (uses Google Cloud Storage).\n",
        "_bucket = 'gs://BUCKET'\n",
        "\n",
        "# Custom container image in Google Container Registry (GCR) to use for training\n",
        "# on Google Cloud AI Platform.\n",
        "_tfx_image = f'gcr.io/{_project_id}/tfx-example-sklearn'\n",
        "\n",
        "# Region to use for Dataflow jobs and AI Platform jobs.\n",
        "#   Dataflow: https://cloud.google.com/dataflow/docs/concepts/regional-endpoints\n",
        "#   AI Platform: https://cloud.google.com/ml-engine/docs/tensorflow/regions\n",
        "_gcp_region = 'us-central1'\n",
        "\n",
        "# A dict which contains the training job parameters to be passed to Google\n",
        "# Cloud AI Platform. For the full set of parameters supported by Google Cloud AI\n",
        "# Platform, refer to\n",
        "# https://cloud.google.com/ml-engine/reference/rest/v1/projects.jobs#Job\n",
        "_ai_platform_training_args = {\n",
        "    'project': _project_id,\n",
        "    'region': _gcp_region,\n",
        "    # Override the default TFX image used for training with one with the correct\n",
        "    # scikit-learn version.\n",
        "    'masterConfig': {\n",
        "        'imageUri': _tfx_image,\n",
        "    },\n",
        "}\n",
        "\n",
        "# A dict which contains the serving job parameters to be passed to Google\n",
        "# Cloud AI Platform. For the full set of parameters supported by Google Cloud AI\n",
        "# Platform, refer to\n",
        "# https://cloud.google.com/ml-engine/reference/rest/v1/projects.models\n",
        "_ai_platform_serving_args = {\n",
        "    'model_name': _pipeline_name,\n",
        "    'project_id': _project_id,\n",
        "    # The region to use when serving the model. See available regions here:\n",
        "    # https://cloud.google.com/ml-engine/docs/regions\n",
        "    # Note that serving currently only supports a single region:\n",
        "    # https://cloud.google.com/ml-engine/reference/rest/v1/projects.models#Model\n",
        "    'regions': [_gcp_region],\n",
        "    # TODO(b/176256164): Update to runtime version 2.4 once that is available\n",
        "    # to align with the version of TF supported by TFX.\n",
        "    # LINT.IfChange\n",
        "    'runtime_version': '2.3',\n",
        "    # LINT.ThenChange(../../../dependencies.py)\n",
        "}\n",
        "\n",
        "# This example assumes that Penguin data is stored in ~/penguin/data and the\n",
        "# utility function is in ~/penguin. Feel free to customize as needed.\n",
        "_penguin_root = os.path.join(_bucket, 'penguin')\n",
        "_data_root = os.path.join(_penguin_root, 'data')\n",
        "\n",
        "# Python module file to inject customized logic into the TFX components.\n",
        "# Trainer requires user-defined functions to run successfully.\n",
        "_trainer_module_file = os.path.join(\n",
        "    _penguin_root, 'experimental', 'penguin_utils_sklearn.py')\n",
        "\n",
        "# Python module file to inject customized logic into the TFX components. The\n",
        "# Evaluator component needs a custom extractor in order to make predictions\n",
        "# using the scikit-learn model.\n",
        "_evaluator_module_file = os.path.join(\n",
        "    _penguin_root, 'experimental', 'sklearn_predict_extractor.py')\n",
        "\n",
        "# Directory and data locations. This example assumes all of the\n",
        "# example code and metadata library is relative to $HOME, but you can store\n",
        "# these files anywhere on your local filesystem. The AI Platform Pusher requires\n",
        "# that pipeline outputs are stored in a GCS bucket.\n",
        "_tfx_root = os.path.join(_bucket, 'tfx')\n",
        "_pipeline_root = os.path.join(_tfx_root, 'pipelines', _pipeline_name)\n",
        "\n",
        "# Pipeline arguments for Beam powered Components.\n",
        "# TODO(b/171316320): Change direct_running_mode back to multi_processing and set\n",
        "# direct_num_workers to 0. Additionally, try to use the Dataflow runner instead\n",
        "# of the direct runner.\n",
        "_beam_pipeline_args = [\n",
        "    '--direct_running_mode=multi_threading',\n",
        "    # 0 means auto-detect based on on the number of CPUs available\n",
        "    # during execution time.\n",
        "    '--direct_num_workers=1',\n",
        "]\n",
        "\n",
        "\n",
        "def _create_pipeline(\n",
        "    pipeline_name: str,\n",
        "    pipeline_root: str,\n",
        "    data_root: str,\n",
        "    trainer_module_file: str,\n",
        "    evaluator_module_file: str,\n",
        "    ai_platform_training_args: Optional[Dict[str, str]],\n",
        "    ai_platform_serving_args: Optional[Dict[str, str]],\n",
        "    beam_pipeline_args: List[str],\n",
        ") -> tfx.dsl.Pipeline:\n",
        "  \"\"\"Implements the Penguin pipeline with TFX.\"\"\"\n",
        "  # Brings data into the pipeline or otherwise joins/converts training data.\n",
        "  example_gen = tfx.components.CsvExampleGen(\n",
        "      input_base=os.path.join(data_root, 'labelled'))\n",
        "\n",
        "  # Computes statistics over data for visualization and example validation.\n",
        "  statistics_gen = tfx.components.StatisticsGen(\n",
        "      examples=example_gen.outputs['examples'])\n",
        "\n",
        "  # Generates schema based on statistics files.\n",
        "  schema_gen = tfx.components.SchemaGen(\n",
        "      statistics=statistics_gen.outputs['statistics'], infer_feature_shape=True)\n",
        "\n",
        "  # Performs anomaly detection based on statistics and data schema.\n",
        "  example_validator = tfx.components.ExampleValidator(\n",
        "      statistics=statistics_gen.outputs['statistics'],\n",
        "      schema=schema_gen.outputs['schema'])\n",
        "\n",
        "  # TODO(humichael): Handle applying transformation component in Milestone 3.\n",
        "\n",
        "  # Uses user-provided Python function that trains a model.\n",
        "  # Num_steps is not provided during evaluation because the scikit-learn model\n",
        "  # loads and evaluates the entire test set at once.\n",
        "  trainer = tfx.extensions.google_cloud_ai_platform.Trainer(\n",
        "      module_file=trainer_module_file,\n",
        "      examples=example_gen.outputs['examples'],\n",
        "      schema=schema_gen.outputs['schema'],\n",
        "      train_args=tfx.proto.TrainArgs(num_steps=2000),\n",
        "      eval_args=tfx.proto.EvalArgs(),\n",
        "      custom_config={\n",
        "          tfx.extensions.google_cloud_ai_platform.TRAINING_ARGS_KEY:\n",
        "          ai_platform_training_args,\n",
        "      })\n",
        "\n",
        "  # Get the latest blessed model for model validation.\n",
        "  model_resolver = tfx.dsl.Resolver(\n",
        "      strategy_class=tfx.dsl.experimental.LatestBlessedModelStrategy,\n",
        "      model=tfx.dsl.Channel(type=tfx.types.standard_artifacts.Model),\n",
        "      model_blessing=tfx.dsl.Channel(\n",
        "          type=tfx.types.standard_artifacts.ModelBlessing)).with_id(\n",
        "              'latest_blessed_model_resolver')\n",
        "\n",
        "  # Uses TFMA to compute evaluation statistics over features of a model and\n",
        "  # perform quality validation of a candidate model (compared to a baseline).\n",
        "  eval_config = tfma.EvalConfig(\n",
        "      model_specs=[tfma.ModelSpec(label_key='species')],\n",
        "      slicing_specs=[tfma.SlicingSpec()],\n",
        "      metrics_specs=[\n",
        "          tfma.MetricsSpec(metrics=[\n",
        "              tfma.MetricConfig(\n",
        "                  class_name='Accuracy',\n",
        "                  threshold=tfma.MetricThreshold(\n",
        "                      value_threshold=tfma.GenericValueThreshold(\n",
        "                          lower_bound={'value': 0.6}),\n",
        "                      change_threshold=tfma.GenericChangeThreshold(\n",
        "                          direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
        "                          absolute={'value': -1e-10})))\n",
        "          ])\n",
        "      ])\n",
        "\n",
        "  evaluator = tfx.components.Evaluator(\n",
        "      module_file=evaluator_module_file,\n",
        "      examples=example_gen.outputs['examples'],\n",
        "      model=trainer.outputs['model'],\n",
        "      baseline_model=model_resolver.outputs['model'],\n",
        "      eval_config=eval_config)\n",
        "\n",
        "  pusher = tfx.extensions.google_cloud_ai_platform.Pusher(\n",
        "      model=trainer.outputs['model'],\n",
        "      model_blessing=evaluator.outputs['blessing'],\n",
        "      custom_config={\n",
        "          tfx.extensions.google_cloud_ai_platform.experimental\n",
        "          .PUSHER_SERVING_ARGS_KEY: ai_platform_serving_args,\n",
        "      })\n",
        "\n",
        "  return tfx.dsl.Pipeline(\n",
        "      pipeline_name=pipeline_name,\n",
        "      pipeline_root=pipeline_root,\n",
        "      components=[\n",
        "          example_gen,\n",
        "          statistics_gen,\n",
        "          schema_gen,\n",
        "          example_validator,\n",
        "          trainer,\n",
        "          model_resolver,\n",
        "          evaluator,\n",
        "          pusher,\n",
        "      ],\n",
        "      enable_cache=True,\n",
        "      beam_pipeline_args=beam_pipeline_args,\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdAffqUQIZIt"
      },
      "source": [
        "#### compile kubeflow pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRnCap-bIl1X"
      },
      "source": [
        "import os\n",
        "from absl import logging\n",
        "\n",
        "from pipeline import configs\n",
        "from pipeline import pipeline\n",
        "from tfx.orchestration.kubeflow.v2 import kubeflow_v2_dag_runner\n",
        "from tfx.proto import trainer_pb2\n",
        "\n",
        "# TFX pipeline produces many output files and metadata. All output data will be\n",
        "# stored under this OUTPUT_DIR.\n",
        "# NOTE: It is recommended to have a separated OUTPUT_DIR which is *outside* of\n",
        "#       the source code structure. Please change OUTPUT_DIR to other location\n",
        "#       where we can store outputs of the pipeline.\n",
        "_OUTPUT_DIR = os.path.join('gs://', configs.GCS_BUCKET_NAME)\n",
        "\n",
        "# TFX produces two types of outputs, files and metadata.\n",
        "# - Files will be created under PIPELINE_ROOT directory.\n",
        "# - Metadata will be written to metadata service backend.\n",
        "_PIPELINE_ROOT = os.path.join(_OUTPUT_DIR, 'tfx_pipeline_output',\n",
        "                              configs.PIPELINE_NAME)\n",
        "\n",
        "# The last component of the pipeline, \"Pusher\" will produce serving model under\n",
        "# SERVING_MODEL_DIR.\n",
        "_SERVING_MODEL_DIR = os.path.join(_PIPELINE_ROOT, 'serving_model')\n",
        "\n",
        "_DATA_PATH = 'gs://{}/tfx-template/data/taxi/'.format(configs.GCS_BUCKET_NAME)\n",
        "\n",
        "def run():\n",
        "  \"\"\"Define a pipeline to be executed using Kubeflow V2 runner.\"\"\"\n",
        "\n",
        "  runner_config = kubeflow_v2_dag_runner.KubeflowV2DagRunnerConfig(\n",
        "      default_image=configs.PIPELINE_IMAGE)\n",
        "\n",
        "  dsl_pipeline = pipeline.create_pipeline(\n",
        "      pipeline_name=configs.PIPELINE_NAME,\n",
        "      pipeline_root=_PIPELINE_ROOT,\n",
        "      data_path=_DATA_PATH,\n",
        "      # TODO(step 7): (Optional) Uncomment here to use BigQueryExampleGen.\n",
        "      # query=configs.BIG_QUERY_QUERY,\n",
        "      preprocessing_fn=configs.PREPROCESSING_FN,\n",
        "      run_fn=configs.RUN_FN,\n",
        "      train_args=trainer_pb2.TrainArgs(num_steps=configs.TRAIN_NUM_STEPS),\n",
        "      eval_args=trainer_pb2.EvalArgs(num_steps=configs.EVAL_NUM_STEPS),\n",
        "      eval_accuracy_threshold=configs.EVAL_ACCURACY_THRESHOLD,\n",
        "      serving_model_dir=_SERVING_MODEL_DIR,\n",
        "    \n",
        "      # Uncomment below to use Dataflow.\n",
        "      # beam_pipeline_args=configs.DATAFLOW_BEAM_PIPELINE_ARGS,\n",
        "    \n",
        "      ai_platform_training_args=configs.GCP_AI_PLATFORM_TRAINING_ARGS,\n",
        "    \n",
        "      # Uncomment below to use Cloud AI Platform.\n",
        "      # ai_platform_serving_args=configs.GCP_AI_PLATFORM_SERVING_ARGS,\n",
        "  )\n",
        "\n",
        "  runner = kubeflow_v2_dag_runner.KubeflowV2DagRunner(config=runner_config)\n",
        "\n",
        "  runner.run(pipeline=dsl_pipeline)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxmPH2akInTH"
      },
      "source": [
        "absl.logging.set_verbosity(absl.logging.INFO)\n",
        "runner_config = tfx.orchestration.experimental.KubeflowDagRunnerConfig(\n",
        "      tfx_image=_tfx_image)\n",
        "\n",
        "tfx.orchestration.experimental.KubeflowDagRunner(config=runner_config).run(\n",
        "      _create_pipeline(\n",
        "          pipeline_name=_pipeline_name,\n",
        "          pipeline_root=_pipeline_root,\n",
        "          data_root=_data_root,\n",
        "          trainer_module_file=_trainer_module_file,\n",
        "          evaluator_module_file=_evaluator_module_file,\n",
        "          ai_platform_training_args=_ai_platform_training_args,\n",
        "          ai_platform_serving_args=_ai_platform_serving_args,\n",
        "          beam_pipeline_args=_beam_pipeline_args))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkKWdyQSI_Rj"
      },
      "source": [
        "#### Or use TFX commandline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEyzzn4OJCvE"
      },
      "source": [
        "!TFX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJH7hj65JI1l"
      },
      "source": [
        "#### kfp also has its own building **function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFrc2H2eJM-n"
      },
      "source": [
        "import kfp\n",
        "import kfp.dsl as dsl\n",
        "import kfp.components as comp\n",
        "from kfp import compiler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkcuAtQOJTtb"
      },
      "source": [
        "#### Seldon core "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVRNaKbnJneu"
      },
      "source": [
        "!s2i"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}